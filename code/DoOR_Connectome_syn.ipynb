{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "bBCw92xxe2op",
        "outputId": "9dd42309-fa42-4884-bfb3-875421efe49d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-1811075398.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "#------------Import Data---------------\n",
        "csv_path_cnt= ('/Connectome.csv')\n",
        "cntset = pd.read_csv(csv_path_cnt)\n",
        "csv_path_ann= ('/DoOR.csv')\n",
        "X_ann = pd.read_csv(csv_path_ann)\n",
        "\n",
        "ff=cntset[cntset[\"directionality\"]=='feedforward']\n",
        "otp=ff[(ff[\"pre_class\"]=='ORN') & (ff[\"post_class\"]=='ALPN')]\n",
        "ptk=ff[(ff['pre_class']=='ALPN') & (ff['post_class']=='KC')]\n",
        "\n",
        "#-----------ORN to ALPN mask: 'mask_otp'-----------------\n",
        "otp_unique = otp[['pre_root_id','post_root_id']]\n",
        "mask_otp_norm = pd.crosstab(\n",
        "    index=otp_unique['pre_root_id'],    # Row: ORN ID\n",
        "    columns=otp_unique['post_root_id']  # Column: ALPN ID\n",
        "    , values=otp['syn_count']           # syn count\n",
        "    , aggfunc='sum'\n",
        "    ).fillna(0)                         #NaN = 0\n",
        "\n",
        "mask_otp = torch.from_numpy(mask_otp_norm.T.values).float().to(device)  # (615, 2278)\n",
        "\n",
        "#------------ORN to ALPN mask: 'mask_otp'-----------------\n",
        "alpn_ids=mask_otp_norm.columns\n",
        "\n",
        "ptk_unique=ptk[['pre_root_id', 'post_root_id']]\n",
        "mask_ptk_norm = (\n",
        "    pd.crosstab(\n",
        "        index=ptk_unique['pre_root_id'],    # Row: ALPN ID\n",
        "        columns=ptk_unique['post_root_id'], # Column: KC ID\n",
        "        values=ptk['syn_count'],\n",
        "        aggfunc='sum'\n",
        "    )\n",
        "    .reindex(index=alpn_ids,   # Make ALPN ID 615\n",
        "             fill_value=0)\n",
        "    .fillna(0)                 # NaN = 0\n",
        ")\n",
        "mask_ptk = torch.from_numpy(mask_ptk_norm.T.values).float().to(device)  # (4907,334)\n",
        "\n",
        "#-----------------------MaskedLinear()----------------------------\n",
        "class MaskedLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, weight):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(weight)\n",
        "        self.bias   = nn.Parameter(torch.zeros(out_features))\n",
        "        self.register_buffer('mask', torch.sign(weight).float())    # +1/-1/0 weight의 부호만 살려서 mask에 저장\n",
        "        nn.init.kaiming_uniform_(self.weight, nonlinearity='relu')\n",
        "    def forward(self, x):\n",
        "        # weight의 절댓값만 학습, 부호는 mask로 고정해서 weight의 부호가 바뀌면서 output 부호가 음수에서 양수가 되는 경우를 방지\n",
        "        w = torch.abs(self.weight) * self.mask\n",
        "        # Use functional linear transformation\n",
        "        return torch.nn.functional.linear(x, w, self.bias)\n",
        "\n",
        "#---------------------Odor Classifier()---------------------------\n",
        "class Odor_classifier(nn.Module):\n",
        "  def __init__(self,input_dim,output_dim, mask1, mask2):\n",
        "    super().__init__()\n",
        "    self.net=nn.Sequential(\n",
        "        MaskedLinear(input_dim,615, mask1),\n",
        "        nn.ReLU(),\n",
        "        MaskedLinear(615,4907, mask2),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4907,output_dim)\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "model = Odor_classifier(input_dim=2278, output_dim=250, mask1=mask_otp, mask2=mask_ptk).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "#-----------------------Train & Test----------------------\n",
        "cls_labels = []\n",
        "for i in range(250):\n",
        "  cls_labels.append(f'Odor_({i+1})')\n",
        "\n",
        "le=LabelEncoder()\n",
        "y_int=le.fit_transform(cls_labels)\n",
        "y = y_int\n",
        "y_out=torch.tensor(y,dtype=torch.long)\n",
        "\n",
        "x_train=[]\n",
        "y_train=[]\n",
        "x_test=[]\n",
        "y_test=[]\n",
        "\n",
        "idx=torch.randint(0,250,(1000,))\n",
        "\n",
        "for n in idx:\n",
        "  noise=torch.normal(mean=0,std=0.2,size=(2278,))\n",
        "\n",
        "  xi=X_ann.iloc[n.item()] # Convert tensor to scalar and use .iloc for integer-based indexing\n",
        "  xn  = torch.zeros(2278, dtype=torch.float32)\n",
        "  xn=torch.tensor(xi.values, dtype=torch.float32)+noise # Convert pandas Series to tensor\n",
        "  x_train.append(xn)\n",
        "  y_train.append(y_out[n]) # Append the individual element y_out[n]\n",
        "\n",
        "idx=torch.randint(0,250,(200,))\n",
        "\n",
        "for k in idx:\n",
        "  noise=torch.normal(mean=0,std=0.2,size=(2278,))\n",
        "\n",
        "  xi=X_ann.iloc[k.item()] # Convert tensor to scalar and use .iloc for integer-based indexing\n",
        "  xn  = torch.zeros(2278, dtype=torch.float32)\n",
        "  xn=torch.tensor(xi.values, dtype=torch.float32)+noise # Convert pandas Series to tensor\n",
        "  x_test.append(xn)\n",
        "  y_test.append(y_out[k]) # Append the individual element y_out[k]\n",
        "\n",
        "x_train = torch.stack(x_train).to(dtype=torch.float32)\n",
        "y_train = torch.stack(y_train).to(dtype=torch.long)\n",
        "x_test = torch.stack(x_test).to(dtype=torch.float32)\n",
        "y_test = torch.stack(y_test).to(dtype=torch.long)\n",
        "\n",
        "train_ds = TensorDataset(x_train, y_train)\n",
        "test_ds  = TensorDataset(x_test,  y_test)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=64, shuffle=False)\n",
        "\n",
        "#---------------Train loop----------------\n",
        "\n",
        "num_epochs = 100\n",
        "train_losses, train_accs = [], []\n",
        "\n",
        "for epoch in trange(num_epochs, desc=\"Epochs\"):\n",
        "    model.train()\n",
        "    tr_loss = tr_acc = 0.0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(xb)\n",
        "        loss   = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        tr_loss += loss.item() * xb.size(0)\n",
        "        tr_acc  += (logits.argmax(dim=1) == yb).float().sum().item()\n",
        "\n",
        "        # for confusion matrix\n",
        "        y_true.append(yb.cpu())\n",
        "        y_pred.append(logits.argmax(dim=1).cpu())\n",
        "\n",
        "    tr_loss /= len(train_loader.dataset)\n",
        "    tr_acc  /= len(train_loader.dataset)\n",
        "    train_losses.append(tr_loss)\n",
        "    train_accs.append(tr_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d}/{num_epochs}  \"\n",
        "          f\"Train Loss: {tr_loss:.4f}, Acc: {tr_acc:.4%}  \")\n",
        "\n",
        "\n",
        "#------------Test Accuracy---------\n",
        "model.eval()\n",
        "te_correct = 0\n",
        "total_samples = 0\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits = model(xb)\n",
        "        te_correct += (logits.argmax(dim=1) == yb).float().sum().item()\n",
        "        total_samples += xb.size(0)\n",
        "test_acc = te_correct / total_samples\n",
        "print(f\"Test set accuracy: {test_acc:.4%}\")\n",
        "\n",
        "\n",
        "#---------Train result plot---------\n",
        "fig, ax1 = plt.subplots(figsize=(8, 4))\n",
        "# Loss: 왼쪽 y축\n",
        "ax1.plot(train_losses, color='tab:blue', label='Train Loss')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss', color='tab:blue')\n",
        "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
        "# Accuracy: 오른쪽 y축\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(train_accs, color='tab:orange', label='Train Acc')\n",
        "ax2.set_ylabel('Accuracy', color='tab:orange')\n",
        "ax2.tick_params(axis='y', labelcolor='tab:orange')\n",
        "\n",
        "lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
        "lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
        "ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc='center right')\n",
        "\n",
        "plt.title('Train Loss & Accuracy')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# ---------Confusion_matrix---------\n",
        "y_true_all = torch.cat(y_true).numpy()\n",
        "y_pred_all = torch.cat(y_pred).numpy()\n",
        "\n",
        "cm = confusion_matrix(y_true_all, y_pred_all)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=False, fmt='d', cmap='Blues')\n",
        "plt.xlabel(\"Predicted label\")\n",
        "plt.ylabel(\"True label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hf74alBXf-Hj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}